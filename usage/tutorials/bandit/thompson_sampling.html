

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Thompson Sampling &mdash; GenRL 0.1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../_static/genrl_cropped.png"/>
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Bayesian" href="bayesian.html" />
    <link rel="prev" title="UCB" href="ucb.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >
          

          
            <a href="../../../index.html">
          

          
            
            <img src="../../../_static/genrl.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/about.html">About</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Tutorials</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Bandit Tutorials</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="bandit_overview.html">Multi Armed Bandit Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="contextual_overview.html">Contextual Bandits Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="ucb.html">UCB</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Thompson Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#using-thompson-sampling-on-a-bernoulli-multi-armed-bandit">Using Thompson Sampling on a Bernoulli Multi-Armed Bandit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="bayesian.html">Bayesian</a></li>
<li class="toctree-l3"><a class="reference internal" href="gradients.html">Gradients</a></li>
<li class="toctree-l3"><a class="reference internal" href="linpos.html">Linear Posterior Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="variational.html">Variational Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="bootstrap.html">Bootstrap</a></li>
<li class="toctree-l3"><a class="reference internal" href="noise.html">Parameter Noise Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="adding_data_bandit.html">Adding a new Data Bandit</a></li>
<li class="toctree-l3"><a class="reference internal" href="adding_dcb_agent.html">Adding a new Deep Contextual Bandit Agent</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Classical/index.html">Classical</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Deep/index.html">Deep RL Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20Custom%20Policies.html">Custom Policy Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20A2C.html">Using A2C</a></li>
<li class="toctree-l2"><a class="reference internal" href="../using_vpg.html">Vanilla Policy Gradient (VPG)</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/agents/index.html">Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/environments/index.html">Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/core/index.html">Core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/utils/index.html">Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/trainers/index.html">Trainers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/common/index.html">Common</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">GenRL</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Tutorials</a> &raquo;</li>
        
          <li><a href="index.html">Bandit Tutorials</a> &raquo;</li>
        
      <li>Thompson Sampling</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/usage/tutorials/bandit/thompson_sampling.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="thompson-sampling">
<h1>Thompson Sampling<a class="headerlink" href="#thompson-sampling" title="Permalink to this headline">¶</a></h1>
<div class="section" id="using-thompson-sampling-on-a-bernoulli-multi-armed-bandit">
<h2>Using Thompson Sampling on a Bernoulli Multi-Armed Bandit<a class="headerlink" href="#using-thompson-sampling-on-a-bernoulli-multi-armed-bandit" title="Permalink to this headline">¶</a></h2>
<p>For an introduction to Multi Armed Bandits, refer to <a class="reference internal" href="bandit_overview.html#bandit-overview"><span class="std std-ref">Multi Armed Bandit Overview</span></a></p>
<p>Thompson Sampling is one of the best methods for solving the Bernoulli
multi-armed bandits problem. It is a ‘sample-based probability matching’
method.</p>
<p>We initially <em>assume</em> an initial distribution(prior) over the quality of
each of the arms. We can model this prior using a Beta distribution,
parametrised by alpha(<span class="math notranslate nohighlight">\(\alpha\)</span>) and beta(<span class="math notranslate nohighlight">\(\beta\)</span>).</p>
<div class="math notranslate nohighlight">
\[PDF = \frac{x^{\alpha - 1} (1-x)^{\beta -1}}{B(\alpha, \beta)}\]</div>
<p>Let’s just think of the denominator as some normalising constant, and
focus on the numerator for now. We initialise <span class="math notranslate nohighlight">\(\alpha\)</span> =
<span class="math notranslate nohighlight">\(\beta\)</span> = 1. This will result in a uniform distribution over the
values (0, 1), making all the values of quality from 0 to 1 equally
probable, so this is a fair initial assumption. Now think of
<span class="math notranslate nohighlight">\(\alpha\)</span> as the number of times we get the reward ‘1’ and
<span class="math notranslate nohighlight">\(\beta\)</span> as the number of times we get ‘0’, for a particular arm.
As our agent interacts with the environment and gets a reward for
pulling any arm, we will update our prior for that arm using Bayes
Theorem. What this does is that it gives a posterior distribution over
the quality, according to the rewards we have seen so far.</p>
<p>At each timestep, we sample the quality: <span class="math notranslate nohighlight">\(Q_t(a)\)</span> for each arm
from the posterior and select the sample with the highest value. The
more an action is tried out, the narrower is the distribution over its
quality, meaning we have a confident estimate of its quality (q(a)). If
an action has not been tried out that often, it will have a more wider
distribution (high variance), meaning we are uncertain about our
estimate of its quality (q(a)). This wider variance of an arm with an
uncertain estimate creates opportunities for it to be picked during
sampling.</p>
<p>As we experience more successes for a particular arm, the value of
<span class="math notranslate nohighlight">\(\alpha\)</span> for that arm increases and similiarly, the more failures
we experience, the value of <span class="math notranslate nohighlight">\(\beta\)</span> increases. Higher the value of
one of the parameters as compared to the other, the more skewed is the
distribution in one of the directions. For eg. if <span class="math notranslate nohighlight">\(\alpha\)</span> = 100
and <span class="math notranslate nohighlight">\(\beta\)</span> = 50, we have seen considerably more successes than
failures for this arm and so our estimate for its quality should be
&gt;0.5. This will be reflected in the posterior of this arm, i.e. the mean
of the distribution, characterised by
<span class="math notranslate nohighlight">\(\frac{\alpha}{\alpha + \beta}\)</span> will be 0.66, which is &gt;0.5 as we
expected.</p>
<p>Code to use Thompson Sampling on a Bernoulli Multi-Armed Bandit:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">genrl.bandit</span> <span class="kn">import</span> <span class="n">BernoulliMAB</span><span class="p">,</span> <span class="n">MABTrainer</span><span class="p">,</span> <span class="n">ThompsonSamplingMABAgent</span>

<span class="n">bandits</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">arms</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="n">reward_probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">bandits</span><span class="p">,</span> <span class="n">arms</span><span class="p">))</span>
<span class="n">bandit</span> <span class="o">=</span> <span class="n">BernoulliMAB</span><span class="p">(</span><span class="n">bandits</span><span class="p">,</span> <span class="n">arms</span><span class="p">,</span> <span class="n">reward_probs</span><span class="p">,</span> <span class="n">context_type</span><span class="o">=</span><span class="s2">&quot;int&quot;</span><span class="p">)</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">ThompsonSamplingMABAgent</span><span class="p">(</span><span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">MABTrainer</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">bandit</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">timesteps</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
</pre></div>
</div>
<p>More details can be found in the docs for
<a class="reference external" href="../../../api/bandit/genrl.core.bandit.html#genrl.core.bandit.bernoulli_mab.BernoulliMAB">BernoulliMAB</a>,
<a class="reference external" href="../../../api/bandit/genrl.agents.bandits.multiarmed.html#module-genrl.agents.bandits.multiarmed.thompson">UCB</a>
and
<a class="reference external" href="../../../api/common/bandit.html#module-genrl.bandit.trainer">MABTrainer</a>.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="bayesian.html" class="btn btn-neutral float-right" title="Bayesian" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="ucb.html" class="btn btn-neutral float-left" title="UCB" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Society for Artificial Intelligence and Deep Learning (SAiDL)

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>