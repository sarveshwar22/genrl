

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Multi Armed Bandit Overview &mdash; GenRL 0.1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../_static/genrl_cropped.png"/>
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Contextual Bandits Overview" href="contextual_overview.html" />
    <link rel="prev" title="Bandit Tutorials" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >
          

          
            <a href="../../../index.html">
          

          
            
            <img src="../../../_static/genrl.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/about.html">About</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Tutorials</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Bandit Tutorials</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Multi Armed Bandit Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#training-an-epsilongreedy-agent-on-a-bernoulli-multi-armed-bandit">Training an EpsilonGreedy agent on a Bernoulli Multi Armed Bandit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="contextual_overview.html">Contextual Bandits Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="ucb.html">UCB</a></li>
<li class="toctree-l3"><a class="reference internal" href="thompson_sampling.html">Thompson Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayesian.html">Bayesian</a></li>
<li class="toctree-l3"><a class="reference internal" href="gradients.html">Gradients</a></li>
<li class="toctree-l3"><a class="reference internal" href="linpos.html">Linear Posterior Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="variational.html">Variational Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="bootstrap.html">Bootstrap</a></li>
<li class="toctree-l3"><a class="reference internal" href="noise.html">Parameter Noise Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="adding_data_bandit.html">Adding a new Data Bandit</a></li>
<li class="toctree-l3"><a class="reference internal" href="adding_dcb_agent.html">Adding a new Deep Contextual Bandit Agent</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Classical/index.html">Classical</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Deep/index.html">Deep RL Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20Custom%20Policies.html">Custom Policy Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20A2C.html">Using A2C</a></li>
<li class="toctree-l2"><a class="reference internal" href="../using_vpg.html">Vanilla Policy Gradient (VPG)</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/agents/index.html">Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/environments/index.html">Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/core/index.html">Core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/utils/index.html">Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/trainers/index.html">Trainers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/common/index.html">Common</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">GenRL</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Tutorials</a> &raquo;</li>
        
          <li><a href="index.html">Bandit Tutorials</a> &raquo;</li>
        
      <li>Multi Armed Bandit Overview</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/usage/tutorials/bandit/bandit_overview.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="multi-armed-bandit-overview">
<span id="bandit-overview"></span><h1>Multi Armed Bandit Overview<a class="headerlink" href="#multi-armed-bandit-overview" title="Permalink to this headline">¶</a></h1>
<div class="section" id="training-an-epsilongreedy-agent-on-a-bernoulli-multi-armed-bandit">
<h2>Training an EpsilonGreedy agent on a Bernoulli Multi Armed Bandit<a class="headerlink" href="#training-an-epsilongreedy-agent-on-a-bernoulli-multi-armed-bandit" title="Permalink to this headline">¶</a></h2>
<p>Multi armed bandits is one of the most basic problems in RL. Think of it
like this, you have ‘n’ levers in front of you and each of these levers
will give you a different reward. For the purposes of formalising the
problem the reward is written down in terms of a reward function i.e.,
the probability of getting a reward when a lever is pulled.</p>
<p>Suppose you try out one of the levers and get a positive reward. What do
you do next? Should you just keep pulling that lever every time or think
what if there might be a better reward to pulling one of the other
levers? This is the exploration - exploitation dilemma.</p>
<p><em>Exploitation</em> - Utilise the information you have gathered till now, to
make the best decision. In this case, after 1 try you know a lever is
giving you a positive reward and you just <em>exploit</em> it further. Since
you do not care about other arms if you keep <em>exploiting</em>, it is known
as the greedy action.</p>
<p><em>Exploration</em> - You explore the untried levers in an attempt to maybe
discover another one which has a higher payout than the one you
currently have some knowledge about. This is exploring all your options
without worrying about the short-term rewards, in hope of finding a
lever with a bigger reward, in the long run.</p>
<p>You have to use an algorithm which correctly trades off exploration and
exploitation as we do not want a ‘greedy’ algorithm which only exploits
and does not explore at all, because there are very high chances that it
will converge to a sub-optimal policy. We do not want an algorithm that
keeps exploring either as this would lead to sub-optimal rewards inspite
of knowing the best action to be taken. In this case, the optimal policy
will be to always pull the lever with the highest reward, but at the
beginning we do not know the probability distribution of the rewards.</p>
<p>So, we want a policy which explores actively at the beginning, building
up an estimate for the reward values(defined as <em>quality</em>) of all the
actions, and then exploiting that from that time onwards.</p>
<p>A Bernoulli Multi-Armed Bandit has multiple arms with each having a
different bernoulli distribution over its reward. Basically each arm has
a probabilty associated with it which is the probability of getting a
reward if that arm is pulled. Our aim is to find the arm which has the
highest probabilty, thus giving us the maximum return.</p>
<p>Notation:</p>
<p><span class="math notranslate nohighlight">\(Q_t(a)\)</span>: Estimated quality of action ‘a’ at timestep ‘t’.</p>
<p><span class="math notranslate nohighlight">\(q(a)\)</span>: True value of action ‘a’.</p>
<p>We want our estimate <span class="math notranslate nohighlight">\(Q_t(a)\)</span> to be as close to the true value
<span class="math notranslate nohighlight">\(q(a)\)</span> as possible, so we can make the correct decision.</p>
<p>Let the action with the maximum quality be <span class="math notranslate nohighlight">\(a^*\)</span>:s</p>
<div class="math notranslate nohighlight">
\[q^* = q(a^*)\]</div>
<p>Our goal is to find this <span class="math notranslate nohighlight">\(q^*\)</span>.</p>
<p>The ‘regret function’ is defined as the sum of ‘regret’ accumulated over
all timesteps. This regret is the cost of not choosing the optimal arm
and instead of exploring. Mathematically it can be written as:</p>
<div class="math notranslate nohighlight">
\[L = E[\sum_{t=0}^T q^* - Q_t(a)]\]</div>
<p>Some policies which are effective at exploring are: 1. <a class="reference external" href="../../../api/bandit/genrl.agents.bandits.multiarmed.html#module-genrl.agents.bandits.multiarmed.epsgreedy">Epsilon
Greedy</a>
2. <a class="reference external" href="../../../api/bandit/genrl.agents.bandits.multiarmed.html#module-genrl.agents.bandits.multiarmed.gradient">Gradient
Algorithm</a>
3. <a class="reference external" href="../../../api/bandit/genrl.agents.bandits.multiarmed.html#module-genrl.agents.bandits.multiarmed.ucb">UCB(Upper Confidence
Bound)</a>
4.
<a class="reference external" href="../../../api/bandit/genrl.agents.bandits.multiarmed.html#module-genrl.agents.bandits.multiarmed.bayesian">Bayesian</a>
5. <a class="reference external" href="../../../bandit/genrl.agents.bandits.multiarmed.html#module-genrl.agents.bandits.multiarmed.thompson">Thompson
Sampling</a></p>
<p>Epsilon Greedy is the most basic exploratory policy which follows a
simple principle to balance exploration and exploitation. It ‘exploits’
the current knowledge of the bandit most of the times, i.e. takes the
action with the largest q value. But with a small probability epsilon,
it also explores a random action. The value of epsilon signifies how
much you want the agent explore. Higher the value, the more it explores.
But remember you do not want an agent to explore too much even after it
has a pretty confident estimate of the reward function, so the value of
epislon should neither be too high nor too low!</p>
<p>For the bandit, you can set the number of bandits, number of arms, and
also reward probabilities of each of these arms seperately.</p>
<p>Code to train an Epsilon Greedy agent on a Bernoulli Multi-Armed Bandit:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">genrl.bandit</span> <span class="kn">import</span> <span class="n">BernoulliMAB</span><span class="p">,</span> <span class="n">EpsGreedyMABAgent</span><span class="p">,</span> <span class="n">MABTrainer</span>

<span class="n">reward_probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">bandits</span><span class="p">,</span> <span class="n">arms</span><span class="p">))</span>
<span class="n">bandit</span> <span class="o">=</span> <span class="n">BernoulliMAB</span><span class="p">(</span><span class="n">arms</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">reward_probs</span><span class="o">=</span><span class="n">reward_probs</span><span class="p">,</span> <span class="n">context_type</span><span class="o">=</span><span class="s2">&quot;int&quot;</span><span class="p">)</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">EpsGreedyMABAgent</span><span class="p">(</span><span class="n">bandit</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">MABTrainer</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">bandit</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">timesteps</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
</pre></div>
</div>
<p>More details can be found in the docs for
<a class="reference external" href="../../../api/bandit/genrl.core.bandit.html#genrl.core.bandit.bernoulli_mab.BernoulliMAB">BernoulliMAB</a>,
<a class="reference external" href="../../../api/bandit/genrl.agents.bandits.multiarmed.html#module-genrl.agents.bandits.multiarmed.epsgreedy">EpsGreedyMABAgent</a>,
<a class="reference external" href="../../../api/common/bandit.html#module-genrl.bandit.trainer">MABTrainer</a>.</p>
<p>You can also refer to the book “Reinforcement Learning: An
Introduction”, Chapter 2 for further information on bandits.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="contextual_overview.html" class="btn btn-neutral float-right" title="Contextual Bandits Overview" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Bandit Tutorials" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Society for Artificial Intelligence and Deep Learning (SAiDL)

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>