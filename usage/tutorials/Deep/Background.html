

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Deep Reinforcement Learning Background &mdash; GenRL 0.1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../_static/genrl_cropped.png"/>
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Vanilla Policy Gradient" href="VPG.html" />
    <link rel="prev" title="Deep RL Tutorials" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >
          

          
            <a href="../../../index.html">
          

          
            
            <img src="../../../_static/genrl.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/about.html">About</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../bandit/index.html">Bandit Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Classical/index.html">Classical</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Deep RL Tutorials</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Deep Reinforcement Learning Background</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#background">Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="#markov-decision-process">Markov Decision Process</a></li>
<li class="toctree-l4"><a class="reference internal" href="#policy-function">Policy Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="#value-function">Value Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="#approximators">Approximators</a></li>
<li class="toctree-l4"><a class="reference internal" href="#objective">Objective</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="VPG.html">Vanilla Policy Gradient</a></li>
<li class="toctree-l3"><a class="reference internal" href="A2C.html">Advantage Actor Critic</a></li>
<li class="toctree-l3"><a class="reference internal" href="PPO.html">Proximal Policy Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20Custom%20Policies.html">Custom Policy Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20A2C.html">Using A2C</a></li>
<li class="toctree-l2"><a class="reference internal" href="../using_vpg.html">Vanilla Policy Gradient (VPG)</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/agents/index.html">Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/environments/index.html">Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/core/index.html">Core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/utils/index.html">Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/trainers/index.html">Trainers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/common/index.html">Common</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">GenRL</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Tutorials</a> &raquo;</li>
        
          <li><a href="index.html">Deep RL Tutorials</a> &raquo;</li>
        
      <li>Deep Reinforcement Learning Background</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/usage/tutorials/Deep/Background.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="deep-reinforcement-learning-background">
<h1>Deep Reinforcement Learning Background<a class="headerlink" href="#deep-reinforcement-learning-background" title="Permalink to this headline">¶</a></h1>
<div class="section" id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<p>The goal of Reinforcement Learning Algorithms is to maximize reward. This is usually achieved by having a policy <span class="math notranslate nohighlight">\(\pi_{\theta}\)</span> perform optimal behavior. Let’s denote this optimal policy by <span class="math notranslate nohighlight">\(\pi_{\theta}^{*}\)</span>. For ease, we define the Reinforcement Learning problem as a Markov Decision Process.</p>
</div>
<div class="section" id="markov-decision-process">
<h2>Markov Decision Process<a class="headerlink" href="#markov-decision-process" title="Permalink to this headline">¶</a></h2>
<p>An Markov Decision Process (MDP) is defined by <span class="math notranslate nohighlight">\((S, A, r, P_{a})\)</span>
where,</p>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S\)</span> is a set of States.</p></li>
<li><p><span class="math notranslate nohighlight">\(A\)</span> is a set of Actions.</p></li>
<li><p><span class="math notranslate nohighlight">\(r : S \rightarrow \mathbb{R}\)</span> is a reward function.</p></li>
<li><p><span class="math notranslate nohighlight">\(P_{a}(s, s')\)</span> is the transition probability that action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(s\)</span> leads to state <span class="math notranslate nohighlight">\(s'\)</span>.</p></li>
</ul>
</div></blockquote>
<p>Often we define two functions, a policy function <span class="math notranslate nohighlight">\(\pi_{\theta}(s,a)\)</span> and <span class="math notranslate nohighlight">\(V_{\pi_{\theta}}(s)\)</span>.</p>
</div>
<div class="section" id="policy-function">
<h2>Policy Function<a class="headerlink" href="#policy-function" title="Permalink to this headline">¶</a></h2>
<p>The policy is the agent’s strategy, we our goal is to make it optimal. The optimal policy is usually denoted by <span class="math notranslate nohighlight">\(\pi_{\theta}^{*}\)</span>. There are usually 2 types of policies:</p>
<div class="section" id="stochastic-policy">
<h3>Stochastic Policy<a class="headerlink" href="#stochastic-policy" title="Permalink to this headline">¶</a></h3>
<p>The Policy Function is a stochastic variable defining a probability distribution over actions given states i.e. likelihood of every action when an agent is in a particular state. Formally,</p>
<div class="math notranslate nohighlight">
\[\pi : S \times A \rightarrow [0,1]\]</div>
<div class="math notranslate nohighlight">
\[a \sim \pi(a|s)\]</div>
</div>
<div class="section" id="deterministic-policy">
<h3>Deterministic Policy<a class="headerlink" href="#deterministic-policy" title="Permalink to this headline">¶</a></h3>
<p>The Policy Function maps from States directly to Actions.</p>
<div class="math notranslate nohighlight">
\[\pi : S \rightarrow A\]</div>
<div class="math notranslate nohighlight">
\[a = \pi(s)\]</div>
</div>
</div>
<div class="section" id="value-function">
<h2>Value Function<a class="headerlink" href="#value-function" title="Permalink to this headline">¶</a></h2>
<p>The Value Function is defined as the expected return obtained when we follow a policy <span class="math notranslate nohighlight">\(\pi\)</span> starting from state S. Usually there are two types of value functions defined State Value Function and a State Action Value Function.</p>
<div class="section" id="state-value-function">
<h3>State Value Function<a class="headerlink" href="#state-value-function" title="Permalink to this headline">¶</a></h3>
<p>The State Value Function is defined as the expected return starting from only State s.</p>
<div class="math notranslate nohighlight">
\[V^{\pi}(s) = E\left[ R_{t} \right]\]</div>
</div>
<div class="section" id="state-action-value-function">
<h3>State Action Value Function<a class="headerlink" href="#state-action-value-function" title="Permalink to this headline">¶</a></h3>
<p>The Action Value Function is defined as the expected return starting from a state s and a taking an action a.</p>
<div class="math notranslate nohighlight">
\[Q^{\pi}(s,a) = E\left[ R_{t} \right]\]</div>
<p>The Action Value Function is also known as the <strong>Quality</strong> Function as it would denote how good a particular action is for a state s.</p>
</div>
</div>
<div class="section" id="approximators">
<h2>Approximators<a class="headerlink" href="#approximators" title="Permalink to this headline">¶</a></h2>
<p>Neural Networks are often used as approximators for Policy and Value Functions. In such a case, we say these are <strong>parameterised</strong> by <span class="math notranslate nohighlight">\(\theta\)</span>. For e.g. <span class="math notranslate nohighlight">\(\pi_{\theta}\)</span>.</p>
</div>
<div class="section" id="objective">
<h2>Objective<a class="headerlink" href="#objective" title="Permalink to this headline">¶</a></h2>
<p>The objective is to choose/learn a policy that will maximize a cumulative function of rewards received at each step, typically the discounted reward over a potential infinite horizon. We formulate this cumulative function as</p>
<div class="math notranslate nohighlight">
\[E\left[{\sum_{t=0}^{\infty}{\gamma^{t} r_{t}}}\right]\]</div>
<p>where we choose an action according to our policy, <span class="math notranslate nohighlight">\(a_{t} = \pi_{\theta}(s_{t})\)</span>.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="VPG.html" class="btn btn-neutral float-right" title="Vanilla Policy Gradient" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Deep RL Tutorials" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Society for Artificial Intelligence and Deep Learning (SAiDL)

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>